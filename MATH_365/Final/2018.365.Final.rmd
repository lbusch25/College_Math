# Math 365 / Comp 365: Final Exam
## *Monday, April 30 - Sunday, May 6 2018*

### <span style="color:red"> Lawson Busch </span> 

### <span style="color:red">I did not work on this assignment with other students.</span> 


The following line sources functions from the class file `365Functions.r`. Feel free to use any of these functions.
```{r, message=FALSE, warning=FALSE}
source("https://drive.google.com/uc?export=download&id=10dNH3VbvxS8Z3OHjP4i9gRbtsf91VVBb") #365functions.R
source("https://drive.google.com/uc?export=download&id=14EU7jxS8LcQ29WOvzAevXigl2wloccqF") #File that loads data for final
require(Matrix)
require(expm)
```

## Problem 1

### Part (1A): Solution

```{r}
#Include code here and then uncomment below 
P = matrix(0, nrow=20, ncol = 20)
for(i in 1:18) {
  for(j in i:(i+2)) {
    if(j == 9) {
      P[i,3] = 1/3
    } else if (j == 8) {
      P[i, 17] = 1/3
    } else if (j == 18) {
      P[i, 12] = 1/3
    } else if(j==6) {
      P[i, 15] = 1/3
    } else {
      P[i,j] = 1/3
    }
  }
}
P[19,19] = 1/3
P[19, 20] = 2/3
P[20,20] = 1
 image(P)
# P
 rowSums(P)
 colSums(P)
```


### Part (1B): Solution

```{r}
#Include code here and then uncomment below 

x20 = rep(0, 20)
x20[1] = 1
Pk = t(P)%^%20
pend = Pk %*% x20
pend



 paste0("The chance of reaching the end is ",format(100*round(pend[20],3)),"%")
 paste0("The unreachable squares are: 6, 8, 9, 10, 11, 18")
```


### Part (1C): Solution

```{r}
#Include code here and then uncomment below 

#Changes to P
P1 = P
P1[19,20] = 0
P1[19, 1] = 2/3
#P1

rowSums(P1)

A = t(P1)
out = eigen(A)
domVec = out$vectors[,1]
percentVec = domVec/sum(domVec)
percentVec

p19 = percentVec[19]
p10 = percentVec[10]
mostlikely = 17
plikely = percentVec[17]

#Re()


 paste0("Probability of being on square 19 is ",format(100*round(p19,3)),"%")
 paste0("Probability of being on square 10 is ",format(100*round(p10,3)),"%")
 paste0("Most likely square is ",mostlikely)
 paste0("Probability of being on most likely square is ",format(100*round(plikely,3)),"%")
```


## Problem 2


### Problem 2a) solution:
```{r}
#Insert your code here, save the three matrices a img0, img1, and img2 respectively, then uncomment below
train0 = train_data[,1:768]
train1 = train_data[,768:1390]
train2 = train_data[,1390:1865]

img0 = matrix(train0[,1], nrow = 16, ncol = 16)
img1 = matrix(train1[,1], nrow = 16, ncol = 16)
img2 = matrix(train2[,1], nrow = 16, ncol = 16)
imPlot(t(img0))
imPlot(t(img1))
imPlot(t(img2))
```

### Problem 2bi. solution:
Each of the systems are Underdetermined systems, as they have more unknowns than equations. It is a bad idea to use least squares on the matrix because the system is underdetermined which implies that there are infinite solutions, as there will be free variables, while we want only one solution.

### Problem 2bii solution:
```{r}
##b is a sample test vector to be classified
##data is the training data to build all of the systems of equations for each digit (i.e. train_data_1000)

##Normal equations
digit_NormLS= function(b,data=train_data_1000){
  bestError = 1000000 #this should update to be the optimal error
  digitrec = 100 #this should update to be the predicted digit between 0-9
  ##Insert your code here
  for(i in 1:10) {
   # A = data[,(i-1):i] #This is probably wrong, need to multiply by 100
    if(i==1) {
      A = data[,1:100]
    } else {
      A = data[,((i-1)*100):(i*100)]
    }
    xStar = solve(t(A)%*%A, t(A)%*%b)
    bStar = A%*%xStar
    r = vnorm(b-bStar)
    if(r<bestError) {
      bestError = r
      digitrec = i-1
    }
  }
  
  return(digitrec)
}

## Least Squares with QR Decomposition

digit_QRLS= function(b,data=train_data_1000){
  bestError = 1000000 #this should update to be the optimal error
  digitrec = 100 #this should update to be the predicted digit between 0-9
  ##Insert your code here
  for(i in 1:10) {
    #A = data[,(i-1):i] #This is probably wrong, need to multiply by 100
    if(i==1) {
      A = data[,1:100]
    } else {
      A = data[,((i-1)*100):(i*100)]
    }
    out = qr(A)
    xStar = solve(qr.R(out), t(qr.Q(out))%*%b)
    bStar = A%*%xStar
    r = vnorm(b-bStar)
    if(r<bestError) {
      bestError = r
      digitrec = i-1
    }
  }
  
  
  return(digitrec)
}

## Least Squares with SVD 

digit_SVDLS= function(b,data=train_data_1000){
  bestError = 1000000 #this should update to be the optimal error
  digitrec = 100 #this should update to be the predicted digit between 0-9
  ##Insert your code here
  for(i in 1:10) {
    #A = data[,(i-1):i] #This here is probably wrong, need to multiply by 100
    if(i==1) {
      A = data[,1:100]
    } else {
      A = data[,((i-1)*100):(i*100)]
    }
    outS = svd(A)
    u = outS$u
    v = outS$v
    d = outS$d
    
    c = t(u)%*%b
    y = c/d
    
    xStar = v %*% y
    
    bStar = A%*%xStar
    r = vnorm(b-bStar)
    if(r<bestError) {
      bestError = r
      digitrec = i-1
    }
  }
  
  return(digitrec)
}
```
	
### Problem 2biii solution:
	
```{r, cache=TRUE}
	#Uncomment lines below
	
Timing = function(method){
  start_time = Sys.time()
  correct = 0
  wrong = 0
  for(i in 1:ncol(test_data[,1:200])){
    RecognizedDigit = method(as.matrix(test_data[,i]))
    if (test_label[i]==RecognizedDigit){
      correct = correct + 1
    }else{
      wrong = wrong + 1
    }
  }
  accuracy = correct / (correct + wrong)
  end_time = Sys.time()
  return(list(time = end_time - start_time, accuracy = accuracy))
}

Timing(digit_NormLS)
Timing(digit_QRLS)
 Timing(digit_SVDLS)
```

```{r}
digit_NormLS(test_data[,1],train_data_1000)
digit_QRLS(test_data[,1],train_data_1000)
digit_SVDLS(test_data[,1],train_data_1000)
```
	
	
The accuracy of each of the methods is exactly the same. However, the timing is significantly different, with the normal least squares method being the fastest by far. Then the QR decomposition, is second fastest and the least squares is the slowest. 
	
### Problem 2ci solution:


```{r}
digit_SVD = function(b, Uj){
  bestError = 1000000
  digitrec = 100
  
  # Put your code here
  J = ncol(Uj)/10
  #print(J)
  n = nrow(Uj)
  
  for(j in 1:10) {
    if(j == 1) {
      PJ = matrix(0, nrow = n, ncol = n)
      for(i in 1:J) {
        PJ = PJ + Uj[,i]%*%t(Uj[,i])
      }
      bHat = PJ%*%b #This is not going to work, rows in PJ dont match b
      r = vnorm(bHat - b)
      if(r < bestError) {
        bestError = r
        digitrec = j-1
      }
    } else {
            PJ = matrix(0, nrow = n, ncol = n)
      for(i in ((j-1)*J):(j*J)) {
        PJ = PJ + Uj[,i]%*%t(Uj[,i])
      }
      bHat = PJ%*%b
      r = vnorm(bHat - b) #might need norm function here
      if(r < bestError) {
        bestError = r
        digitrec = j-1
      }
    }
  }
  
  return(digitrec)
}

```
		
	
### Problem 2cii solution:
```{r}
#J = 17
U_100=c() #Find this matrix
for(i in 1:10) {
  if(i == 1) {
    A = train_data_1000[,1:100]
  } else {
    A = train_data_1000[,((i-1)*100):(i*100)] #Will need to change thes indices
  }
  outS = svd(A)
  u = outS$u
  U_100 = cbind(U_100, u[,1:17])
}
```
	
### Problem 2ciii solution:
```{r}
U_J=c()#Find this matrix
num = ncol(train_data)/10
for(i in 1:10) {
  if(i == 1) {
    A = train_data[,1:num]
  } else {
    A = train_data[,((i-1)*num):(i*num)] #Will need to change thes indices
  }
  outS = svd(A)
  u = outS$u
  U_J = cbind(U_J, u[,1:17])
}
```

### Problem 2civ solution:
```{r}
# #Uncommment code below


Timing_SVD = function(U,datapoints = ncol(test_data)){ #Remove indexing
  start_time = Sys.time()
  correct = 0
  wrong = 0
  for(i in 1:datapoints){
    RecognizedDigit = digit_SVD(as.matrix(test_data[,i]),U)
    if (test_label[i]==RecognizedDigit){
      correct = correct + 1
    }else{
      wrong = wrong + 1
    }
  }
  accuracy = correct / (correct + wrong)
  end_time = Sys.time()
  return(list(time = end_time - start_time, accuray = accuracy))
}

Timing_SVD(U_100)
Timing_SVD(U_J)
```

```{r}
digit_SVD(test_data[,1],U_100)
```

Compared to the methods aboce, this method is more accurate as it has about 93.2% accuracy as opposed to 92.5% accuracy of the other three methods. However, it takes way more time to run as it is on the order of several minutes as opposed to seconds, so I would say that in general digit_SVD's increased accuracy is not worth the additional time.

## Problem 3


```{r}
#Data is saved as spiral, for instance
plot(spiral)
```

### Problem 3a solution:
```{r}
s <- function(x1, x2, alpha=1) {
  #Insert code here
  return(exp(-alpha*vnorm(x1-x2)^2))
}
s(spiral[1,], spiral[2,])
```

### Problem 3b solution:
```{r}
make.similarity <- function(my.data) {  
  # fill in your code here.
  n = nrow(my.data)
  S = matrix(0, nrow = n, ncol = n)
  for(i in 1:n) {
    for(j in 1:n) {
      S[i,j] = s(my.data[i,], my.data[j,])
    }
  }
  return(S)
}
```

### Problem 3c solution:

```{r}

#Comment the code below
make.affinity <- function(S, n.neighbors=2) {
  N <- length(S[,1]) #Number of elements in a row

  if (n.neighbors >= N) {
    A <- S #If smaller than specified, A = S
  } else {
    A <- matrix(0, ncol=N, nrow=N) #A is a matrix of specified size
    for(i in 1:N) { 
      closest.similarities <- sort(S[i,], decreasing=TRUE)[1:n.neighbors] #Finds closest neighbors by the number specified
      for (s in closest.similarities) {
        j <- which(S[i,] == s) #Find each corresponding index (and thus element) to s in S
        A[i,j] <- S[i,j] #Set Similarity to be that of S[i,j]
        A[j,i] <- S[i,j] #Set same similarity going the other way
      } 
    }
  }
  return(A)  
}
```

### Problem 3d solution:

```{r}
#Compute S
S=make.similarity(spiral)

#Compute A
A=make.affinity(S,3)

#Compute D
D=diag(rowSums(A))

#Compute L
L=D-A

#Compute eigenvalues and eigenvectors of L
out = eigen(L)
out$values
#out

#Find Z
Z = out$vectors[,99:100]


#Uncomment and run code below
k=2 #replace
library(stats)
km <- kmeans(Z, centers=k, nstart=5)
plot(spiral, col=km$cluster)
```

Two of the eigenvalues of L are 0. 

## Problem 4

```{r}
set.seed(365)
y = rnorm(5000)
a = hist(y,breaks=9)
(heights = a$counts)
(centers = a$mids)
(t = a$breaks)
```

### Problem 4a solution:

```{r}
#Fill in code here
xx = seq(-4, 4, length=1000)
natCub = splinefun(centers, heights, method="fmm")
y1 = natCub(xx)

set.seed(365)
y = rnorm(5000)
a = hist(y,breaks=9)
(heights = a$counts)
(centers = a$mids)
(t = a$breaks)
lines(xx, y1, col="red")
```

### Problem 4bi solution:

Area under each hist block should just be the height, since we are talking a width of 1. 

```{r}
#Insert code here and uncomment
newHeights = c(0, heights) #Since height at -5 = 0
Fvals = cumsum(newHeights)
t = a$breaks

print(Fvals)

```

### Problem 4bii solution:

```{r}
#Insert code here 
require(pracma)

pp <- cubicspline(t, Fvals, endp2nd=TRUE, der=c(0, 0))
F <- function(t) {ppval(pp, t)}

xpoints = seq(-5, 4, length = 1000)
plot(xpoints, F(xpoints))
```

### Problem 4biii solution:

```{r}
#Insert code here 
f = FDDeriv(F)
xx = seq(-4, 4, length=1000)
y2 = f(xx)

set.seed(365)
y = rnorm(5000)
a = hist(y,breaks=9)
(heights = a$counts)
(centers = a$mids)
(t = a$breaks)
lines(xx, y1, col="red")
lines(xx, y2, col="blue")
```

The PDF interpolation method gives us a better fit than the fmm interpolation method, as it does not curl up at the end of the graph. This means that it avoids increasing the area under the curve at the end of the function, making it a more accurate fit.

## Problem 5
```{r}
source("https://drive.google.com/uc?export=download&id=14EU7jxS8LcQ29WOvzAevXigl2wloccqF") #File that loads data for final
```
Here is the digram frequency matrix of Lincoln's Gettysburg address:
```{r}
print(G)
```

### Problem 5a solution:

Here is the vector $f=G1$ for part (a):
```{r}
letters = c('A','B','C','D','E','F','G','H','I','J','K','L','M',
            'N','O','P','Q','R','S','T','U','V','W','X','Y','Z')
f = G%*%rep(1,26)
t = cbind(letters,f)
print(t)
```

This vector tells us how many times each letter starts a two letter combination. This is because it is the sum of all $ij$ pairs for each letter, and thus represents how many times the letter was immediately followed by another letter.

### Problem 5b solution:

```{r}
#Include your commands here and uncomment below
outG = svd(G)
outG$d
rankG = 23
paste0("The rank of G is: ", rankG)
```
The rank of G is 23, which makes sense since three rows (x, z, and j) are zero rows in the data set, implying that we will have at least 3 free variables and thus a matrix that is not of full rank. We can tell this because the d value for the svd decomposition has 3 zero eigenvalues.

### Problem 5c solution:

```{r}
#Insert code here and uncomment below
SVDApprox = function(A, k){
  out = svd(A)
  u = out$u
  v = out$v
  o = out$d
  
  #Ak = o[1:k]*u[,1:k]%*%t(v[,1:k])
  
  for(i in 1:k) {
    if(i == 1) {
      Ak = o[1]*u[,1]%*%t(v[,1])
    } else {
      Ak = Ak + o[i]*u[,i]%*%t(v[,i])
    }
  }
  return(Ak)
}
G1 = SVDApprox(G, 1)
G2 = SVDApprox(G, 2)
image(G1)
image(G2)
```

### Problem 5d solution:


```{r}
#Insert code here 
y = outG$v[,2]
x = outG$u[,2]
plot(x, y)
text(x, y, label = letters)
```


From this graph, we can see that the vowels are gouped in the upper left corner, away from the consonants clustered in the middle of the graph. This is likely because vowels are more likely to appear before other letters than consonants, as their $v$ values are higher than zero while their $u$ values are lower than zero. This implies that $ij$ pairings are more common for vowels than consonants.

### Problem 5e solution:

Here is the scrambled matrix for part (e):
```{r}
print(S)
out = svd(S)
y = outG$v[,2]
x = outG$u[,2]
plot(-x, -y)
text(-x, -y, label = letters)
-x
-y

#Insert code here
paste0("The vowels are positions: 4, 14, 18, 19, 20")
```

### Problem 5f solution (EXTRA CREDIT):

INCLUDE YOUR DECODED MESSAGE HERE

INCLUDE YOUR DESCRIPTION HERE





## Optional Extra Credit Problem 
```{r}
myEigen=function(A){
  #Insert code here
  return(list(vals=E,vecs=V))
}
```

You can test your function on this matrix $A$:
```{r}
(A = rbind(c(1,2,3,4),c(2,3,6,5),c(3,6,1,7),c(4,5,7,0)))
```

