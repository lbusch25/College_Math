---
title: "Homework 3"
author: "Lawson Busch"
date: "9/20/2018"
output: html_document
---

```{r}
library(ggplot2)
library(dplyr)
library(ggridges)

joy <- read.csv("https://www.macalester.edu/~ajohns24/data/joy_division.csv")
```

## Excercise 1

### 1a)

```{r}
joy_pop <- joy %>%
  arrange(desc(track_popularity))
head(joy_pop)

joy_least <- joy %>%
  arrange(track_popularity)
head(joy_least)
```

The most popular Joy Division track on Spotify is Disorder - 2007 Remastered Version, and the least popular Joy Division track on Spotify is Colony.

### 1b)

```{r}
ggplot(joy, aes(x = track_popularity)) + geom_histogram(binwidth=5, color = 'white')

ggplot(joy, aes(x = track_number, y = track_popularity)) + geom_point() + geom_smooth(method = 'lm')

ggplot(joy, aes(x = track_popularity, fill = album_name)) + geom_density(alpha = 0.5)

ggplot(joy, aes(x = track_number, y = track_popularity, color = album_name)) + geom_point() + geom_smooth(method = 'lm')
```

### 1c)

```{r}
ggplot(joy, aes(x = track_popularity, y = album_name)) +
    geom_density_ridges() + 
    theme_ridges()
```

### 1d)

The overall takeaway from these plots is that Joy Division's most popular album is Unknown Pleasures, its least popular album is Les Bains Douches, and Closer falls in the middle of these two albums. All of Joy Divisions most popular songs come from the Unknown Pleasures album. We also see that the higher the track number on an album, the lower the overall popularity (in general).

## Excercise 2

```{r}
joy_2 <- joy %>%
  select(-album_name, -album_release_date, -track_name)
head(joy_2)
dim(joy_2)
```

## Excercise 3

```{r}
library(leaps)
```

### 3a)

```{r}
#Do backward stepwise selection for all 12 possible predictors
back_step <- regsubsets(track_popularity ~ ., joy_2, method = "backward", nvmax = 12)
# Store the summary information
back_summary <- summary(back_step)
back_summary
```

A plot of the adjusted $R^2$ values for the backward stepwise selection models.
```{r}
plot_data <- data.frame(size = 1:12, adjr2 = back_summary$adjr2)
ggplot(plot_data, aes(x = size, y = adjr2)) + 
    geom_point() + 
    geom_line() + 
    labs(x="subset size", y="Adjusted R-squared")
```

### 1b)

There is a drastic drop off in adjusted $R^2$ values once you reach seven predictors because the penalties for adding more predictors get stronger, and at that point the statistical significance of the predictors gets weaker, causing a drop off as the penalty for adding a predictor outweighs the predictors value.

### 1c)

I would say that the optimal subset selection is with six predictor values, as that is both the last added predictor where we see a significant increase in adjusted $R^2$ value and is the last predictor before the drop off in adjusted $R^2$ value begins. While you could argue for a simpler model, I think the increase in adjusted $R^2$ is worth the additional predictors up until the sixth predictor.

```{r}
coef(back_step, 6)
```

The six predictors used for this subset are track_number, energy, loudness, modeminor, liveness, and valence.

## Excercise 4

### Excercise 4a)

```{r}
# Perform best subsets
best_subsets <- regsubsets(track_popularity ~ ., joy_2, nvmax = 12)

# Store the summary information
best_summary <- summary(best_subsets)
best_summary
```

And a plot of our adjusted $R^2$ values for best subset selection:

```{r}
plot_data <- data.frame(size = 1:12, adjr2 = best_summary$adjr2)
ggplot(plot_data, aes(x = size, y = adjr2)) + 
    geom_point() + 
    geom_line() + 
    labs(x="subset size", y="Adjusted R-squared")
```

### 4b)

From the above plot, we can again see that the optimal subset size is 6 predictors, as it is the last significant increase in adjusted $R^2$ value, and we can see that after this increase, the adjusted $R^2$ values actually begin to drop off again.

```{r}
coef(best_subsets, 6)
```

We can see that the six predictors included in the model are track_number, energy, loudness, modeminor, liveness, and valence, which are the exact same predictors found by the backward stepwise method.

### 4c)

The optimal subsets chosen by the backward stepwise and the best subset procedures agree, as they are the exact same subsets with the same adjusted $R^2$ value. 

## Excercise 5

### 5a)

```{r}
joy_model <- lm(track_popularity ~ track_number + energy + loudness + mode + liveness + valence, joy_2)
summary(joy_model)
```

The estimated model formula is:

$$trackpopularity = -1.03*tracknumber-12.9*energy+2.2*loudness+5.2*modeminor-12.4*liveness-9.3*valence$$

### 5b)

To write the most popular song possible, joy divison should primarily focus on putting the track early in the album (low track_number) and making it as loud as possible (high loudness). These two predictors should be focused on as they are the most statistically significant (assuming all other factors held constant). Then, Joy Division should focus on minimizing energy, liveness, and valence, while maximizing modeminor (again assuming all other factors are constant when adjusting the value for one predictor).

## Excercise 6

### 6a)

As you can see from the summary output above, the $R^2$ for my model is 0.77.

### 6b)

My loop to calculate LOOCV for my model:
```{r}
mspe_total = rep(0, 34)
for(i in 1:34) {
  train <- joy_2[-i,]
  test <- joy_2[i,]
  
  model_train <- lm(track_popularity ~ track_number + energy + loudness + mode + liveness + valence, train)
  model_test <- predict(model_train, newdata = data.frame(track_number = test$track_number, energy = test$energy,
                                                          loudness = test$loudness, mode = test$mode, liveness = test$liveness,
                                                          valence = test$valence))
  
  test_observed <- test$track_popularity
  test_residuals <- test_observed - model_test
  
  mspe_test = mean(test_residuals^2)
  mspe_total[i] = mspe_test
}

loocv = mean(mspe_total)
loocv
```

And checking using the cv.glm function:
```{r}
library(boot)
joy_model_glm <- glm(track_popularity ~ track_number + energy + loudness + mode + liveness + valence, joy_2, family = "gaussian")
joy_model_cv <- cv.glm(joy_2, joy_model_glm)
joy_model_cv$delta
```

And we can see that my loop produces the correct LOOCV.

### 6c)

While the model that uses all of the predictors has a higher $R^2$ value than my model (0.83 vs 0.77), it has a higher LOOCV than my model (52.23 vs 28.09). I would argue that my model is still better because the model with all of the possible predictors has a higher LOOCV, and as we saw in previous analysis as we include more than six predictors the adjusted $R^2$ values begin to drop. This implies that while the model including all predictors has a better $R^2$ value, it is most likely overfit and will not translate as well to other data. 

### 6d)

This model meets the independance assumption because the observations for each track do not depend on the observations for any other track. 

First to check the observation of trend and homoskedasticity we can plot the residuals of the model against the predictions:

```{r}
joy_output <- data.frame(
  predicted = joy_model$fitted,
  residual = joy_model$residuals)

ggplot(joy_output, aes(y = residual, x = predicted)) + 
    geom_point() + 
    geom_hline(yintercept = 0)
```

From this plot we can see that the assumption of trend holds for this model, as the points appear to be evenly scattered around zero. We can also see that the assumption of homoskedasticity holds for this model, as the residual values never appear to diverge away from zero.

To check the assumption of normality we can create a q-q plot of the residuals:

```{r}
# Q-Q plot of residuals
ggplot(joy_output, aes(sample = residual)) + 
    geom_qq()
```

And from this output we can see that the assumption of normality also holds for this model, as the points roughly resemble a strait line, which is what we would expect if normality holds true.

## Excercise 7

### 7a)

```{r}
set.seed(2018)
library(glmnet)

joy_2 %>% na.omit(joy_2)

# First process the data!  
x <- model.matrix(track_popularity ~ ., joy_2)[,-1]
y <- joy_2$track_popularity

# Fit the LASSO for a grid of lambda
lasso_mod <- glmnet(x, y, alpha = 1)

# Plot a summary of these models
plot(lasso_mod, xvar = "lambda", label = TRUE)
```

### 7b)

Looking at the above plot, if we used a $log \lambda = 1$, then we would have $3$ predictors remaining in the model.

### 7c)

```{r}
set.seed(2018)

# Calculate the CV error rate for the LASSOs at each lambda
lasso_cv <- cv.glmnet(x, y, alpha = 1)

# Plot the CV results
plot(lasso_cv)
```

In this plot we see the cv value dip initially as we begin to eliminate predictors with higher lambda values. At a certain point, as we continue to eliminate predictors, our model becomes less accurate (hence a higher cv value), but we remove more predictors, so the trade off is that the model is simpler. Another reason that the cv begins to increase is that as we increase the lambda value, our coefficients shrink further, allowing for more residual error.

## Excercise 8

### 8a)

```{r}
lambda_best <- lasso_cv$lambda.min
lambda_best
log(lambda_best)
```

The exact lambda for which the LASSO CV error is minimized is 0.5282913. We also see that the log of this value matches what we would expect it to be from the aboce plot.

### 8b)

```{r}
# Grab the coefficients for the LASSO using this lambda
lasso_coef <- predict(lasso_mod, type = "coefficients", s = lambda_best)
lasso_coef
```

The estimated model formula for the LASSO method is:

$$trackpopularity = 78.51 -0.91*tracknumber-9.58*energy+1.89*loudness+2.67*modeminor-9.74*liveness-5.38*valence$$

### 8c)

All of the variables in this model overlap with the variables in my final subsets model. However, when looking at the coefficients, we see that while all of the signs are the same, the values for every coefficient are shrunk. This makes sense, as the LASSO method is a shrinkage and selection method. So the shrunk coefficients are what we would expect to see.